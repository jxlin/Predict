{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_set = pd.read_csv('TimeBasedFeatures-Dataset-15s-AllinOne.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in combined_set.columns: # Loop through all columns in the dataframe\n",
    "    if combined_set[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        combined_set[feature] = pd.Categorical(combined_set[feature]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = combined_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled = data2.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = shuffled.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = shuffled.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainsize = int(len(shuffled['class1']) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testsize = len(shuffled['class1']) - trainsize\n",
    "npredictors = len(predictors.columns)\n",
    "noutputs = 1\n",
    "numiter = 10000\n",
    "modelfile = '/tmp/trained_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (15006, 23) (15006, 7)\n",
      "Validation set (1876, 23) (1876, 7)\n",
      "Test set (1876, 23) (1876, 7)\n",
      "Training set (15006, 23) (15006, 7)\n",
      "Validation set (1876, 23) (1876, 7)\n",
      "Test set (1876, 23) (1876, 7)\n"
     ]
    }
   ],
   "source": [
    "num_labels = 7\n",
    "image_size = 23\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.values\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(predictors[:trainsize], targets[:trainsize])\n",
    "valid_dataset, valid_labels = reformat(predictors[trainsize:trainsize + testsize / 2], targets[trainsize:trainsize + testsize / 2])\n",
    "test_dataset, test_labels = reformat(predictors[trainsize + testsize / 2:], targets[trainsize + testsize / 2:])\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subset = 15006\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :].astype(np.float32))\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset.astype(np.float32))\n",
    "  tf_test_dataset = tf.constant(test_dataset.astype(np.float32))\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 15968755.000000\n",
      "Training accuracy: 30.9%\n",
      "Validation accuracy: 26.4%\n",
      "Initialized\n",
      "Loss at step 0: 15968755.000000\n",
      "Training accuracy: 30.9%\n",
      "Validation accuracy: 26.4%\n",
      "Loss at step 100: 3065381126144.000000\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 32.5%\n",
      "Loss at step 100: 3065381126144.000000\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 32.5%\n",
      "Loss at step 200: 15248485515264.000000\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 41.6%\n",
      "Loss at step 200: 15248485515264.000000\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 41.6%\n",
      "Loss at step 300: 4114921029632.000000\n",
      "Training accuracy: 48.7%\n",
      "Validation accuracy: 48.3%\n",
      "Loss at step 300: 4114921029632.000000\n",
      "Training accuracy: 48.7%\n",
      "Validation accuracy: 48.3%\n",
      "Loss at step 400: 13390432960512.000000\n",
      "Training accuracy: 55.1%\n",
      "Validation accuracy: 24.1%\n",
      "Loss at step 400: 13390432960512.000000\n",
      "Training accuracy: 55.1%\n",
      "Validation accuracy: 24.1%\n",
      "Loss at step 500: 4474676969472.000000\n",
      "Training accuracy: 50.0%\n",
      "Validation accuracy: 42.7%\n",
      "Loss at step 500: 4474676969472.000000\n",
      "Training accuracy: 50.0%\n",
      "Validation accuracy: 42.7%\n",
      "Loss at step 600: 4964877860864.000000\n",
      "Training accuracy: 45.3%\n",
      "Validation accuracy: 56.4%\n",
      "Loss at step 600: 4964877860864.000000\n",
      "Training accuracy: 45.3%\n",
      "Validation accuracy: 56.4%\n",
      "Loss at step 700: 9989921964032.000000\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 37.7%\n",
      "Loss at step 700: 9989921964032.000000\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 37.7%\n",
      "Loss at step 800: 3553349074944.000000\n",
      "Training accuracy: 43.8%\n",
      "Validation accuracy: 53.4%\n",
      "Test accuracy: 53.3%\n",
      "Loss at step 800: 3553349074944.000000\n",
      "Training accuracy: 43.8%\n",
      "Validation accuracy: 53.4%\n",
      "Test accuracy: 53.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "with tf.Session(graph=graph) as session: \n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "train_dataset = train_dataset.astype(np.float32)\n",
    "valid_dataset = valid_dataset.astype(np.float32)\n",
    "test_dataset = test_dataset.astype(np.float32)\n",
    "with graph.as_default():\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 41860256.000000\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 48.9%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 41860256.000000\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 48.9%\n",
      "Minibatch loss at step 500: 26188967837696.000000\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 47.7%\n",
      "Minibatch loss at step 500: 26188967837696.000000\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 47.7%\n",
      "Minibatch loss at step 1000: 14405175607296.000000\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 24.8%\n",
      "Minibatch loss at step 1000: 14405175607296.000000\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 24.8%\n",
      "Minibatch loss at step 1500: 5069009321984.000000\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 42.1%\n",
      "Minibatch loss at step 1500: 5069009321984.000000\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 42.1%\n",
      "Minibatch loss at step 2000: 7049229369344.000000\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 47.8%\n",
      "Minibatch loss at step 2000: 7049229369344.000000\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 47.8%\n",
      "Minibatch loss at step 2500: 5404002091008.000000\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 51.3%\n",
      "Minibatch loss at step 2500: 5404002091008.000000\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 51.3%\n",
      "Minibatch loss at step 3000: 5115023458304.000000\n",
      "Minibatch accuracy: 55.5%\n",
      "Validation accuracy: 47.3%\n",
      "Test accuracy: 48.5%\n",
      "Minibatch loss at step 3000: 5115023458304.000000\n",
      "Minibatch accuracy: 55.5%\n",
      "Validation accuracy: 47.3%\n",
      "Test accuracy: 48.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
